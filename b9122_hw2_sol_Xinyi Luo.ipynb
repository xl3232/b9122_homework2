{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "beda462c",
   "metadata": {},
   "source": [
    "# Homework 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e788cb64",
   "metadata": {},
   "source": [
    "## Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736f8456",
   "metadata": {},
   "source": [
    "### 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f2b68af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "#from urllib.request import Request\n",
    "\n",
    "#Verify whether a url contains word covid or not\n",
    "def contain_covid(url):\n",
    "    req = urllib.request.Request(url,headers={'User-Agent': 'Mozilla/5.0'})\n",
    "    page = urllib.request.urlopen(req)\n",
    "    soup = BeautifulSoup(page.read())\n",
    "    con = False\n",
    "    for tag in soup.find_all(\"p\"):\n",
    "        t = tag.get_text(separator=\" \")\n",
    "        t = t.lower()\n",
    "        t = t.replace(\"-\",\" \")\n",
    "        if 'covid' in t:\n",
    "            con = True\n",
    "            break\n",
    "    return con"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e43fbf35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with url=['https://www.federalreserve.gov/newsevents/pressreleases.htm']\n",
      "List of URLs contains covid:\n",
      "https://www.federalreserve.gov/newsevents/pressreleases/2021-press.htm\n",
      "https://www.federalreserve.gov/newsevents/pressreleases/2020-press.htm\n",
      "https://www.federalreserve.gov/newsevents/pressreleases/monetary20220615a.htm\n",
      "https://www.federalreserve.gov/newsevents/pressreleases/monetary20220504a.htm\n",
      "https://www.federalreserve.gov/newsevents/pressreleases/monetary20220126a.htm\n",
      "https://www.federalreserve.gov/newsevents/pressreleases/other20220523a.htm\n",
      "https://www.federalreserve.gov/newsevents/pressreleases/enforcement20220405a.htm\n",
      "https://www.federalreserve.gov/newsevents/pressreleases/other20220225a.htm\n",
      "https://www.federalreserve.gov/newsevents/pressreleases/bcreg20220214a.htm\n",
      "https://www.federalreserve.gov/newsevents/pressreleases/other20220114a.htm\n",
      "https://www.federalreserve.gov/newsevents/pressreleases/monetary20211215a.htm\n",
      "https://www.federalreserve.gov/newsevents/pressreleases/monetary20211103a.htm\n",
      "https://www.federalreserve.gov/newsevents/pressreleases/monetary20210922a.htm\n",
      "https://www.federalreserve.gov/newsevents/pressreleases/monetary20210616a.htm\n",
      "https://www.federalreserve.gov/newsevents/pressreleases/monetary20210428a.htm\n",
      "https://www.federalreserve.gov/newsevents/pressreleases/monetary20210317a.htm\n",
      "https://www.federalreserve.gov/newsevents/pressreleases/monetary20210127a.htm\n"
     ]
    }
   ],
   "source": [
    "#course code\n",
    "seed_url = \"https://www.federalreserve.gov/newsevents/pressreleases.htm\"\n",
    "\n",
    "urls = [seed_url]    #queue of urls to crawl\n",
    "seen = [seed_url]    #stack of urls seen so far\n",
    "opened = []          #we keep track of seen urls so that we don't revisit them\n",
    "covid_con = []       #the list of url that contains the word \"covid\"\n",
    "\n",
    "maxNumUrl = 1000; #set the maximum number of urls to visit\n",
    "print(\"Starting with url=\"+str(urls))\n",
    "while len(urls) > 0 and len(opened) < maxNumUrl and len(covid_con) < 15:\n",
    "    # DEQUEUE A URL FROM urls AND TRY TO OPEN AND READ IT\n",
    "    try:\n",
    "        curr_url=urls.pop(0)\n",
    "        #print(\"num. of URLs in stack: %d \" % len(urls))\n",
    "        #print(\"Trying to access= \"+curr_url)\n",
    "        req = urllib.request.Request(curr_url,headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        webpage = urllib.request.urlopen(req).read()\n",
    "        opened.append(curr_url)\n",
    "\n",
    "    except Exception as ex:\n",
    "        print(\"Unable to access= \"+curr_url)\n",
    "        print(ex)\n",
    "        continue    #skip code below\n",
    "\n",
    "    # IF URL OPENS, CHECK WHICH URLS THE PAGE CONTAINS\n",
    "    # ADD THE URLS FOUND TO THE QUEUE url AND seen\n",
    "    soup = BeautifulSoup(webpage)  #creates object soup\n",
    "    # Put child URLs into the stack\n",
    "    for tag in soup.find_all('a', href = True): #find tags with links\n",
    "        childUrl = tag['href'] #extract just the link\n",
    "        o_childurl = childUrl\n",
    "        childUrl = urllib.parse.urljoin(seed_url, childUrl)\n",
    "        #print(\"seed_url=\" + seed_url)\n",
    "        #print(\"original childurl=\" + o_childurl)\n",
    "        #print(\"childurl=\" + childUrl)\n",
    "        #print(\"seed_url in childUrl=\" + str(seed_url in childUrl))\n",
    "        #print(\"Have we seen this childUrl=\" + str(childUrl in seen))\n",
    "        if seed_url[:-5] in childUrl and childUrl not in seen:\n",
    "            #print(\"***urls.append and seen.append***\")\n",
    "            urls.append(childUrl)\n",
    "            seen.append(childUrl)\n",
    "            if contain_covid(childUrl) == True:\n",
    "                covid_con.append(childUrl)\n",
    "\n",
    "print(\"List of URLs contains covid:\")\n",
    "for url in covid_con:\n",
    "    print(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80294853",
   "metadata": {},
   "source": [
    "### 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0629ee67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verify whether a url contains word covid or not\n",
    "\n",
    "def contain_charges(url):\n",
    "    req = urllib.request.Request(url,headers={'User-Agent': 'Mozilla/5.0'})\n",
    "    page = urllib.request.urlopen(req)\n",
    "    soup = BeautifulSoup(page.read(), features=\"lxml\")\n",
    "    con = False\n",
    "    #for tag in soup.find_all(\"p\"):\n",
    "     #   t = tag.get_text(separator=\" \")\n",
    "    t = soup.get_text()\n",
    "    text = t.lower()\n",
    "    if 'charges' in text:\n",
    "        con = True\n",
    "    return con"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3550b49f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with url=['https://www.sec.gov/news/pressreleases']\n",
      "List of URLs contains charge and its text:\n",
      "URL:\n",
      "https://www.sec.gov/news/pressreleases#main-content\n",
      "Text:\n",
      "\n",
      "      Skip to main content\n",
      "    \n",
      "URL:\n",
      "https://www.sec.gov/news/pressreleases.rss\n",
      "Text:\n",
      "Press Releases\n",
      "URL:\n",
      "https://www.sec.gov/news/pressrelease/2017-64.html\n",
      "Text:\n",
      "Executives Charged With Manipulating Company's Accounting Systems to Steal Money\n",
      "URL:\n",
      "https://www.sec.gov/news/pressrelease/2017-63.html\n",
      "Text:\n",
      "SEC Charges Firms Involved in Layering, Manipulation Schemes\n",
      "URL:\n",
      "https://www.sec.gov/news/pressrelease/2017-62.html\n",
      "Text:\n",
      "SEC Charges Marijuana-Related Company and Executives With Touting Bogus Revenues\n",
      "URL:\n",
      "https://www.sec.gov/news/pressrelease/2017-60.html\n",
      "Text:\n",
      "SEC Charges Mexico-Based Homebuilder in $3.3 Billion Accounting Fraud \n",
      "URL:\n",
      "https://www.sec.gov/news/pressrelease/2017-52.html\n",
      "Text:\n",
      "SEC Staff Issues Guidance Update and Investor Bulletin on Robo-Advisers\n",
      "URL:\n",
      "https://www.sec.gov/news/pressrelease/2017-44.html\n",
      "Text:\n",
      "SEC Charges Chinese Citizens Who Reaped Massive Profits From Insider Trading on Comcast-Dreamworks Acquisition\n",
      "URL:\n",
      "https://www.sec.gov/news/pressrelease/2016-270.html\n",
      "Text:\n",
      "Company Settles Charges in Whistleblower Retaliation Case\n",
      "URL:\n",
      "https://www.sec.gov/news/pressrelease/2016-252.html\n",
      "Text:\n",
      "PIMCO Settles Charges of Misleading Investors About ETF Performance\n",
      "URL:\n",
      "https://www.sec.gov/news/pressrelease/2016-250.html\n",
      "Text:\n",
      "SEC Charges Asset Management Fund and Manager \n",
      "URL:\n",
      "https://www.sec.gov/news/pressrelease/2016-102.html\n",
      "Text:\n",
      "Brokerage Firm Charged With Anti-Money Laundering Failures\n",
      "URL:\n",
      "https://www.sec.gov/news/pressrelease/2016-49.html\n",
      "Text:\n",
      "SEC Charges Oregon-Based Investment Group and Executives With Defrauding Investors\n",
      "URL:\n",
      "https://www.sec.gov/news/pressrelease/2016-26.html\n",
      "Text:\n",
      "SEC Names C. Dabney Oâ€™Riordan and Alka Patel as Associate Regional Directors in Los Angeles Office\n",
      "URL:\n",
      "https://www.sec.gov/news/pressrelease/2015-216.html\n",
      "Text:\n",
      "SEC to Hold Equity Market Structure Advisory Committee Meeting on October 27\n",
      "URL:\n",
      "https://www.sec.gov/news/pressrelease/2015-195.html\n",
      "Text:\n",
      "SEC Charges Florida-Based CPA with Fraud for Issuing Bogus Audit Opinions\n",
      "URL:\n",
      "https://www.sec.gov/news/pressrelease/2015-189.html\n",
      "Text:\n",
      "SEC Announces Fraud Charges in Cross-Border Scheme to Secretly Control and Manipulate Stock of Chinese Companies After Reverse Mergers\n",
      "URL:\n",
      "https://www.sec.gov/news/pressrelease/2015-163.html\n",
      "Text:\n",
      "SEC Charges 32 Defendants in Scheme to Trade on Hacked News Releases\n",
      "URL:\n",
      "https://www.sec.gov/news/pressrelease/2015-62\n",
      "Text:\n",
      "SEC Charges Oregon-Based Defense Contractor With FCPA Violations\n",
      "URL:\n",
      "https://www.sec.gov/news/pressrelease/2015-13\n",
      "Text:\n",
      "SEC Charges Former Executive at Tampa-Based Engineering Firm With FCPA Violations\n",
      "URL:\n",
      "https://www.sec.gov/news/pressrelease/2012-2012-103htm.html\n",
      "Text:\n",
      "SEC Halts Fraudulent Investment Scheme by New York-Based Fund Manager\n"
     ]
    }
   ],
   "source": [
    "seed_url1 = \"https://www.sec.gov/news/pressreleases\"\n",
    "\n",
    "urls1 = [seed_url1]    #queue of urls to crawl\n",
    "seen1 = [seed_url1]    #stack of urls seen so far\n",
    "opened1 = []          #we keep track of seen urls so that we don't revisit them\n",
    "charge_con = []       #the list of url that contains the word \"covid\"\n",
    "\n",
    "maxNumUrl1 = 1000; #set the maximum number of urls to visit\n",
    "print(\"Starting with url=\"+str(urls1))\n",
    "while len(urls1) > 0 and len(opened1) < maxNumUrl1 and len(charge_con) < 20:\n",
    "    # DEQUEUE A URL FROM urls AND TRY TO OPEN AND READ IT\n",
    "    try:\n",
    "        curr_url1=urls1.pop(0)\n",
    "        #print(\"num. of URLs in stack: %d \" % len(urls))\n",
    "        #print(\"Trying to access= \"+curr_url)\n",
    "        req1 = urllib.request.Request(curr_url1,headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        webpage1 = urllib.request.urlopen(req1).read()\n",
    "        opened1.append(curr_url1)\n",
    "\n",
    "    except Exception as ex:\n",
    "        print(\"Unable to access= \"+curr_url1)\n",
    "        print(ex)\n",
    "        continue    #skip code below\n",
    "\n",
    "    # IF URL OPENS, CHECK WHICH URLS THE PAGE CONTAINS\n",
    "    # ADD THE URLS FOUND TO THE QUEUE url AND seen\n",
    "    soup1 = BeautifulSoup(webpage1, features=\"lxml\")  #creates object soup\n",
    "    # Put child URLs into the stack\n",
    "    for tag1 in soup1.find_all('a', href = True): #find tags with links\n",
    "        childUrl1 = tag1['href'] #extract just the link\n",
    "        o_childurl1 = childUrl1\n",
    "        title = tag1.text\n",
    "        childUrl1 = urllib.parse.urljoin(seed_url1, childUrl1)\n",
    "        #print(\"seed_url=\" + seed_url)\n",
    "        #print(\"original childurl=\" + o_childurl)\n",
    "        #print(\"childurl=\" + childUrl)\n",
    "        #print(\"seed_url in childUrl=\" + str(seed_url in childUrl))\n",
    "        #print(\"Have we seen this childUrl=\" + str(childUrl in seen))\n",
    "        if seed_url1[:-1] in childUrl1 and childUrl1 not in seen1:\n",
    "            #print(\"***urls.append and seen.append***\")\n",
    "            urls1.append(childUrl1)\n",
    "            seen1.append(childUrl1)\n",
    "            if contain_charges(childUrl1) == True:\n",
    "                l = [childUrl1, title]\n",
    "                charge_con.append(l)\n",
    "\n",
    "print(\"List of URLs contains charge and its text:\")\n",
    "for url1 in charge_con:\n",
    "    print(\"URL:\")\n",
    "    print(url1[0])\n",
    "    print(\"Text:\")\n",
    "    print(url1[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ab53be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
